{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a47a8",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36a44bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path\n",
    "import opendatasets as od\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff1d93",
   "metadata": {},
   "source": [
    "# Setup device agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c3ee02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448a8d3",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db638a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset already exists. Skipping download....\n"
     ]
    }
   ],
   "source": [
    "if Path(\".\\\\mood-image-dataset\").is_dir():\n",
    "    print(\"The dataset already exists. Skipping download....\")\n",
    "else:\n",
    "    print(\"Downloading the dataset\")\n",
    "    od.download(\"https://www.kaggle.com/datasets/lavishalakhmani/mood-image-dataset?select=images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3be682",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\".\\\\mood-image-dataset\\\\images\\\\train\")\n",
    "test_dir = Path(\".\\\\mood-image-dataset\\\\images\\\\validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdca02",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72473c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab740ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 26921\n",
      "Length of test dataset = 7066\n",
      "Shape of image: torch.Size([3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(train_dir,transform)\n",
    "test_data = datasets.ImageFolder(test_dir,transform)\n",
    "print(f\"Length of train dataset = {len(train_data)}\")\n",
    "print(f\"Length of test dataset = {len(test_data)}\")\n",
    "img,label=train_data[0]\n",
    "print(f\"Shape of image: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "383ba98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = train_data.classes\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11478a",
   "metadata": {},
   "source": [
    "# Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe5d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1e612db80d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1e612fc0450>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader=DataLoader(dataset=train_data,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False)\n",
    "train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a906d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 842 batches of 32\n",
      "Length of testing dataloader: 221 batches of 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of training dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of testing dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530fb20",
   "metadata": {},
   "source": [
    "# Visulalizing Images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d69048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 48, 48])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "example_img,example_label=next(iter(train_dataloader))\n",
    "print(example_img.shape)\n",
    "print(example_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a4f74fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = torch.randint(0,len(example_label),size=[1]).item()\n",
    "img,label=example_img[random_idx],example_label[random_idx]\n",
    "# img=img.permute(1,2,0)\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.title(class_labels[label])\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91213161",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b47913ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoodDetector(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1440, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MoodDetector(nn.Module):\n",
    "    def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*12*12,out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x:torch.tensor):\n",
    "#         print(x.shape)\n",
    "        x = self.block1(x)\n",
    "#         print(x.shape)\n",
    "        x = self.block2(x)\n",
    "#         print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "\n",
    "model = MoodDetector(input_shape=3,hidden_units=10,output_shape=len(class_labels)).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f987b",
   "metadata": {},
   "source": [
    "# Setting up loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bc2458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf75dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414622e",
   "metadata": {},
   "source": [
    "# Functionating the Training and Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0e9540aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, data_loader, loss_fn, optimizer, accuracy_fn, epoch, device):\n",
    "#     model.train()\n",
    "#     train_loss, train_acc = 0, 0\n",
    "#     for X, y in data_loader:\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "#         y_pred = model(X)\n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         train_loss += loss.item()\n",
    "#         train_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     train_loss /= len(data_loader)\n",
    "#     train_acc /= len(data_loader)\n",
    "#     if epoch % 5 == 0:\n",
    "#         print(f\"Epoch {epoch}: Train loss: {train_loss:.5f}, Train accuracy: {train_acc:.2f}%\")\n",
    "        \n",
    "        \n",
    "# def test(data_loader, model, loss_fn, accuracy_fn, epoch, device):\n",
    "#     model.eval()\n",
    "#     test_loss, test_acc = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in data_loader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             y_pred = model(X)\n",
    "#             test_loss += loss_fn(y_pred, y).item()\n",
    "#             test_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "#     test_loss /= len(data_loader)\n",
    "#     test_acc /= len(data_loader)\n",
    "#     if epoch % 5 == 0:\n",
    "#         print(f\"Epoch {epoch}: Test loss: {test_loss:.5f}, Test accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8acb6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model:torch.nn.Module,\n",
    "#           data_loader:torch.utils.data.DataLoader,\n",
    "#           loss_fn:torch.nn.Module,\n",
    "#           optimizer:torch.optim.Optimizer,\n",
    "#           accuracy_fn,\n",
    "#           epoch:int,\n",
    "#           device: torch.device = device):\n",
    "#     train_loss,train_acc = 0,0\n",
    "#     model.to(device)\n",
    "    \n",
    "#     for batch, (X,y) in enumerate(data_loader):\n",
    "#         X,y = X.to(device),y.to(device)\n",
    "        \n",
    "#         y_pred = model(X)\n",
    "        \n",
    "#         loss = loss_fn(y_pred,y)\n",
    "#         train_loss += loss\n",
    "#         train_acc += accuracy_fn(y,y_pred.argmax(dim=1))\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     train_loss /= len(data_loader)\n",
    "#     train_acc /= len(data_loader)\n",
    "#     if epoch%5==0:\n",
    "#         print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "            \n",
    "# def test(data_loader: torch.utils.data.DataLoader,\n",
    "#          model: torch.nn.Module,\n",
    "#          loss_fn: torch.nn.Module,\n",
    "#          accuracy_fn,\n",
    "#          epoch:int,\n",
    "#          device: torch.device = device):\n",
    "#     test_loss, test_acc = 0, 0\n",
    "#     model.to(device)\n",
    "#     model.eval() # put model in eval mode\n",
    "#     # Turn on inference context manager\n",
    "#     with torch.inference_mode():\n",
    "#         for X, y in data_loader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "\n",
    "#             test_pred = model(X)\n",
    "\n",
    "#             test_loss += loss_fn(test_pred, y)\n",
    "#             test_acc += accuracy_fn(y_true=y,\n",
    "#                 y_pred=test_pred.argmax(dim=1)\n",
    "#             )\n",
    "#         test_loss /= len(data_loader)\n",
    "#         test_acc /= len(data_loader)\n",
    "#         if epoch%5==0:\n",
    "#             print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "537ee02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:torch.nn.Module,\n",
    "          data_loader:torch.utils.data.DataLoader,\n",
    "          loss_fn:torch.nn.Module,\n",
    "          optimizer:torch.optim.Optimizer,\n",
    "          epoch:int,\n",
    "          accuracy_fn,\n",
    "          device:torch.device=device\n",
    "         ):\n",
    "    model.to(device)\n",
    "    model.train() # Setting the model to training mode\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch, (X,y) in enumerate(data_loader): # By using this function we are accessing the images and their labels from dataset\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        # Make the forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Calculate the loss and accuracy\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        y_pred_class=y_pred.argmax(dim=1)\n",
    "        acc=accuracy_fn(y,y_pred_class)\n",
    "        \n",
    "        # Accumulate the loss and accuracy\n",
    "        train_loss+=loss\n",
    "        train_acc+=acc\n",
    "        \n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #Optimizer step \n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculating the average training loss and accuracy\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        print(f\"Train Loss: {loss:.2f} | Average Train Loss: {train_loss:.2f} | Train Accuracy: {acc:.2f} | Average Train Accuracy: {train_acc:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8192569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:torch.nn.Module,\n",
    "        data_loader:torch.utils.data.DataLoader,\n",
    "        loss_fn:torch.nn.Module,\n",
    "        accuracy_fn,\n",
    "        epoch:int,\n",
    "        device:torch.device=device):\n",
    "    model.to(device)\n",
    "    model.eval() # Setting the model to testing mode\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(data_loader):\n",
    "            X,y = X.to(device),y.to(device)\n",
    "            \n",
    "            # Make the forward Pass\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Calculate the loss and accuracy \n",
    "            loss = loss_fn(y_pred,y)\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            acc = accuracy_fn(y,y_pred_class)\n",
    "            \n",
    "            # Accumulate the loss and accuracy\n",
    "            test_loss += loss\n",
    "            test_acc +=acc\n",
    "        \n",
    "        # Calculating the average testing loss and accuracy\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        if epoch%5==0:\n",
    "            print(f\"Test Loss: {loss:.2f} | Average Test Loss: {test_loss:.2f} | Test Accuracy: {acc:.2f} | Average Test Accuracy: {test_acc:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5458174",
   "metadata": {},
   "source": [
    "# Function for getting training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dd6f99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float,end:float,device: torch.device=True):\n",
    "  \"\"\"Prints differecnce between start and end time\"\"\"\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636d4da",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "65e03776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(train_dataloader,test_dataloader,model,accuracy_fn,loss_fn,optimizer,device):\n",
    "    train_time_start = timer()\n",
    "    epochs = 30\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        if epoch%5==0:\n",
    "            print(f\"Epoch: {epoch}\\n---------\")\n",
    "\n",
    "        train(data_loader=train_dataloader,\n",
    "             model=model,\n",
    "             loss_fn=loss_fn,\n",
    "             optimizer=optimizer,\n",
    "             accuracy_fn=accuracy_fn,\n",
    "             epoch=epoch,\n",
    "             device=device)\n",
    "\n",
    "        test(data_loader=test_dataloader,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            epoch=epoch,\n",
    "            device=device)\n",
    "\n",
    "    train_time_end=timer()\n",
    "    print_train_time(train_time_start,train_time_end,device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "66a39732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ba91e8f2cc4e7aa517ddead6a85317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train Loss: 2.31 | Average Train Loss: 1.80 | Train Accuracy: 11.11 | Average Train Accuracy: 25.48\n",
      "Test Loss: 2.20 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Epoch: 5\n",
      "---------\n",
      "Train Loss: 1.79 | Average Train Loss: 1.80 | Train Accuracy: 22.22 | Average Train Accuracy: 25.49\n",
      "Test Loss: 2.26 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Epoch: 10\n",
      "---------\n",
      "Train Loss: 1.82 | Average Train Loss: 1.80 | Train Accuracy: 22.22 | Average Train Accuracy: 25.49\n",
      "Test Loss: 2.22 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Epoch: 15\n",
      "---------\n",
      "Train Loss: 1.63 | Average Train Loss: 1.80 | Train Accuracy: 33.33 | Average Train Accuracy: 25.50\n",
      "Test Loss: 2.31 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Epoch: 20\n",
      "---------\n",
      "Train Loss: 1.80 | Average Train Loss: 1.80 | Train Accuracy: 22.22 | Average Train Accuracy: 25.49\n",
      "Test Loss: 2.22 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Epoch: 25\n",
      "---------\n",
      "Train Loss: 1.79 | Average Train Loss: 1.80 | Train Accuracy: 33.33 | Average Train Accuracy: 25.50\n",
      "Test Loss: 2.28 | Average Test Loss: 1.81 | Test Accuracy: 0.00 | Average Test Accuracy: 25.81\n",
      "Train time on cuda: 1089.145 seconds\n"
     ]
    }
   ],
   "source": [
    "mood_detector = loop(train_dataloader=train_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    model=model,\n",
    "                    loss_fn=loss_fn,\n",
    "                    accuracy_fn=accuracy_fn,\n",
    "                    optimizer=optimizer,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\".\\\\\")\n",
    "MODEL_PATH.mkdir(parents=True,exist_ok=True)\n",
    "MODEL_NAME = 'mood_detector.pth'\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "print(f\"Saving model to:- {MODEL_SAVE_PATH}\")\n",
    "torch.save(mood_detector,MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
